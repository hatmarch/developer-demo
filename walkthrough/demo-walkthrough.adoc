= Developer Demo 
:experimental:
:imagesdir: ../images
:toc:
:toclevels: 4

[IMPORTANT]
.On necessary operators
====
See link:../README.adoc[] for information on operators and other prerequisites that must be installed for the demo to run properly.
====

This demo centers around the creation of a serverless payment service that supports the "Coolstore" website the relevant architecture of which can be seen here:

image:coolstore-arch.png[]

== Setup Tips Prior to Walkthrough ==

* Open CodeReadyWorkspaces with the devfile to ensure it is initialized at least once
* VSCode in the Desktop to the right of the main demo desktop
* Setup your windows something like this:
+
image:scaling-desktop-setup.png[]
+
* For every shell referenced run the following commands
.. Go to the root of the `developer-demo` directory on your local machine
.. Run the following docker command
+
----
docker run -it -v ~/.kube:/home/jboss/.kube -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/developer-demo quay.io/mhildenb/dev-demo-shell /bin/zsh
----
+
.. Then run the following command to setup your environment
+
----
cd /workspaces/developer-demo
. scripts/shell-setup.sh
----

== Coolstore Introduction

[NOTE]
****
NOTE: [red]#Hide any "share screen" notification before running through the demo#
****

. Start with `$PROJECT_PREFIX`-dev project, which should have the Red Hat Coolstore setup minus the payment service
. It will currently have two services failing, this will be due to a kafka cluster not having been deployed

== Operators and Creation of Kafka Cluster ==

=== Cluster Creation ===

. Start with operators are installed on the cluster, such as those that are key to the demonstration.  This include:
* Serverless Operator
* Pipelines Operator
* Kafka Operator
* Kafka Event Source
. Go to the dev-demo-support project
. Click on `AMQStreams` operator
. Select the Kafka Tab and click "Create Kafka"
+
image:create-kafka.png[]
+
. Show the UI form for creation
** Show brokers
** Show zookeeper options
+
image:kafka-form.png[]
+
. Instead of filling out through the form, just use the YAML view
** [blue]#Mention that there are link:https://kafka.apache.org/documentation/[tons of different configuration options for kafka] and they are no all listed in the UI#
. Add the following under the `spec:kafka:config` to ensure maximum scaling [red]#_(for later in the demo)_#
+
----
    num.partitions: 100
----
+
. You will be returned to the Kakfas list.  Select the new "my-cluster" entry, then select YAML as there is a customization we need to add that isn't available in the UI
. Then select "Resources" and scroll through the list
+
image:kafka-resources.png[]
+
. *Whilst waiting for the cluster to come up:*
. Go back to the main Operator page and point out that there are other Kafka resources that can be created via custom resources, such as `Topic` s
. [blue]#Mention that the AMQStreams operator acts to create custom resources for any topics that are created programmatically and that it is also possible to create topics using custom resources#
** If you want to demonstrate the creation of topics from a CR, see <<Creating Topics in the UI, here>>

NOTE: whilst you're waiting for the Kafka cluster to come up, you can start to move on to the next section

=== Revisit the Coolstore (with Kafka queue) ===

. Go to the `dev-demo-dev` project and show the coolstore app in the "Topology View" of the "Developer Perspective"
. Launch the coolstore from the badge on the coolstore service on the dev perspective
** Can use this to contextualize with the above diagram (or from these slides link:https://docs.google.com/presentation/d/1XtvEx9cMRqrlMcY_EdiIsBR78WJawoSfXvFiyt66pS4/edit#slide=id.g72cacdd2b4_0_120[here])

. Setup watch for the two topics by First open a shell watch window to run the following command to see ORDERS coming in
+
----
oc exec -c kafka my-cluster-kafka-0 -n dev-demo-support -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic orders
----
+
. Show PAYMENTS with this command in another shell
+
----
oc exec -c kafka my-cluster-kafka-0 -n dev-demo-support -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
[IMPORTANT]
****
NOTE: Before moving on to next step, you need to ensure the order and cart service are running at this point before actually purchasing.

You can run this command to see if the topics have been created appropriately:
----
oc get kt -n dev-demo-support -w
----
****
+
. Pick any item and purchase it
. Point out that only the `orders` queue changes
. Show the orders in the website and that our order has not yet been processed

== Knative Serving: Create Payment Service ==

Next we'll create our knative payment service that will take our orders and process them.

. Create Service: Developer Perspective: Add knative payment service
.. Click Add, select image
.. Image is: `quay.io/mhildenb/coolstore-payment-java:latest`
.. Select knative service
.. Name the service `payment`
.. Customize Scaling
*** Target and Max concurrency to 1
.. Customize Deployment
*** Environment: 
**** *Name*: `mp.messaging.outgoing.payments.bootstrap.servers`
**** *Value*: `my-cluster-kafka-bootstrap.dev-demo-support:9092` [red]#_or use config map for value, see below_#
+
image:payment-env.png[]
+
*** Label: `app.kubernetes.io/part-of=focus`
+

_OPTIONAL_: Show that the payment service can now be invoked via URL

== Knative Eventing: Create Event Source ==

Next we need to wire the payment service to an event source, in our case the `orders` topic

. From the topology view, go the the knative payment service
. From the payment service on the topology view, pull out blue arrow
. Select `Event Source` from end of arrow
. Fill in the following Kafka SOURCE:
+
image:kafka-source.png[]
+
. Show the service starting up from the topology view
. Refresh the order details page on coolstore.  Order should now be processed

. Go back to the Coolstore site and buy something
. Point out that only the orders queue changes AS WELL AS payments queue
. Show the orders in the website and that our most recent order has been processed

== Serverless: Scaling Support ==

Let's innundate the payment service with calls to see how it responds to the concurrency targets we set.  For the fastest services out there (such as quarkus native compilations) you are probably best off using the kafka spammer.  In a shell run the following commands to effectively download the spammer into the project and then rsh into it

[blue]#EXPLAIN: Because our service is so fast we need simulate entries coming in all at once, hence we'll use a tool called `kafka-spammer` to put (bogus) messages on the topic concurrently and see how our pods scale#

. Go to the `dev-demo-dev` project and show the coolstore app in the "Topology View" of the "Developer Perspective"
. Setup the following options on the developer perspective:
** Display Options > Check the `Pod Count` box
** Application > Select `Focus` to show only the services relevant to the coolstore
. First show how whenever we put something on the order topic it spins up the service (do this from the third [blue] shell)
+
----
oc exec -it -c kafka my-cluster-kafka-0 -n ${PROJECT_PREFIX}-support -- /opt/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic orders
----
+
. Enter a couple items and see how a pod spins up (but it fails)
** It processes orders too quickly to spin up concurrent requests.  Let's see what happens when we spam it
. Cancel the producer window and instead download the "kafka spammer" into the proper project:
+
----
oc -n ${PROJECT_PREFIX}-support run kafka-spammer --image=quay.io/rhdevelopers/kafkaspammer:1.0.2 --env "mp.messaging.outgoing.mystream.topic=orders"
----
+
. Wait for the spammer to be ready
+
----
oc get pods -n dev-demo-support -w
----
+
. Run the following to find the pod and effectively rsh into it:
+
----
KAFKA_SPAMMER_POD=$(oc -n ${PROJECT_PREFIX}-support get pod -l "run=kafka-spammer" -o jsonpath='{.items[0].metadata.name}')
oc -n ${PROJECT_PREFIX}-support exec -it $KAFKA_SPAMMER_POD -- /bin/sh
----
. Once on the pod, you can `curl` localhost using a path input that defines the number of concurrent requests you want to post to the topic.
** NOTE: Keep requests to 50 or lower and pods should scale as expected given the concurrently limits set on the knative service.  Much more than this and other factors (speed of processing, vicissitudes of kafka and eventing) cause fewer than expected pods to spin up
+
----
NUM_REQUESTS=50
# send 50 concurrent posts to the order topic
curl localhost:8080/${NUM_REQUESTS}
----

[IMPORTANT]
.If Running a Live Demo
****
[red]#Jump to the first few steps of <<Inner Loop: Intro to CodeReady Workspaces,this section>> to start the creation of the CodeReady Workspace.  The workspace can take some time to complete, so whilst it's building, you can <<Create Tekton Pipeline with Builder,Create the Tekton Pipeline>> in parallel.#
****

== Inner Loop: Intro to CodeReady Workspaces ==

We actually have this service setup on a local git repo.  This git repo triggers a pipeline that we have created in the cicd project.  To tell openshift about this, we need to update our payment service with some annotations

. First, go to the codeready project and show the installed operator
** could make up a conceit that this is from CRW
. Then navigate back to the Developer Perspective and center in on the payment service
. Run the following command in the shell
** NOTE: The uri is referencing an https endpoint as that is the only way CRW recognizes devfiles
----
kn service update payment --annotation "app.openshift.io/vcs-ref=master" --annotation "app.openshift.io/vcs-uri=https://github.com/hatmarch/coolstore.git" --revision-name "{{.Service}}-{{.Generation}}" -n dev-demo-dev
----
. If the command was successful, a badge should now appear indicating that the service is recognized as one that can be edited with CRW
** [blue]#This would normally come in when using S2I (git repo) but this functionality isn't currently exposed for knative services in the UI#
+
image:crw-badge.png[]
+
. Click on the badge and CRW should start loading (possibly after you login and create a CRW with your OCP credentials)
. In the meantime, in a new tab, navigate to the url in the annotation uri and show the devfile
** scroll through the devfile to explain at a high level the contents
+
. Log into CodeReady Workspaces
** includes giving name and email
. You will now be presented the building screen.  This will take a few minutes

[IMPORTANT]
.If running a live demo
****
[red]#Whilst you wait for the development environment to spin up, you can go back and <<Create Tekton Pipeline with Builder,Create the Tekton Pipeline>>.  By the time that section is demonstrated the build should be complete#
****

== Create Tekton Pipeline with Builder ==

NOTE: These instructions assume a `PROJECT_PREFIX` of `dev-demo`

Now let's say we want to create a little pipeline to deploy our service to staging.  We want the pipeline to do two things:

. Tag the current dev latest version with a version number in staging
. Create a new knative service in staging that points to that newly created tag

Here on the instructions on how we'll do this

. From the `dev-demo-dev` project, open the Pipeline Builder
. Name the pipeline `deploy-staging`
. Create a new parameter called `VERSION`
. Create a new image resource called `stage-image`
. Create a new task of type `openshift-client-local`
. Enter the following arguments on the command
+
----
oc tag -n dev-demo-stage --reference-policy=local dev-demo-dev/payment:latest dev-demo-stage/payment-stage:$(params.VERSION)
----
+
. Be sure to point out the `$(params.VERSION)`
. Then hit the plus to the right of the oc task
. Select the `kn-service` ClusterTask and fill in the args accordingly in the image below
** For ease of pasting, the environmental arg is *with no quotes*
** [red]#Because of a bug in this version of pipeline builder, you must set a value for array arguments, if you don't a `''` will get sent through and they deploy will fail#
+
----
mp.messaging.outgoing.payments.bootstrap.servers=my-cluster-kafka-bootstrap.dev-demo-support:9092
----
+
. Your pipeline should look like this:
+
image:pipeline-builder-kn-service.png[]
+
. Run the pipeline filling it in as follows:
** NOTE: Enter a new image resource that points to the *staging image stream*
+
----
image-registry.openshift-image-registry.svc:5000/dev-demo-stage/payment-stage
----
+
image:oc-start-pipeline-ui.png[]
+
. Click the "Logs" tab to watch it run
. It should complete this time.  When the logs indicate it has finished, go to the Developer Perspective of the dev-demo-stage
. Click on the payment-service and get the route that was created and paste it into value `KN_ROUTE`
. Then run this command to call the route (whilst watching payment queue from before)
+
----
hey -n 100 -c 100 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_ROUTE
----

== Inner Loop to Outer Loop: Trigger Pipeline from CRW

. kbd:[CMD+P] and `PaymentResource` to open that file quickly
. Edit the file
** Add to SUCCESS and FAILURE messages a `(CRW Build)`
. Go to the git panel
. Select files for checkin
. Add message and checkin
. Push to master and login
** gogs
** gogs
. Open the Pipelines drawer of the `dev-demo-cicd` project
. Go to pipeline runs
. Show the pipeline in progress
. When the pipeline completes, prove that the payment service has been updated either by:
.. Purchasing something in the coolstore and looking at the payment queue
.. Setting `KN_ROUTE` to the payment service route and invoking with this command:
+
----
curl -X POST -H "Content-Type: application/json" -d @$DEMO_HOME/example/order-payload.json $KN_ROUTE
----

[IMPORTANT]
.If running a live demo
****
[red]#If time allows, you can show more features of CodeReady Workspaces by clicking <<Extended CRW,here>>.#
****



== Troubleshooting ==

=== Event Source:

If the Developer Perspective doesn't let you create an Event source by giving you a screen with this error:
----
Creation of event sources are not currently supported on this cluster
----
image:event-sources-error.png[]

Assuming that all the necessary operators and CRs are installed, you can force the system to update itself like this:

. Create a knative service (such as payment)
. Create a kafka event source via yaml file, like this:
** NOTE: You may need to edit the bootstrap server for your demo (e.g. add the .dev-demo-support subdomain to the service)
+
----
oc apply -f $DEMO_HOME/coolstore/payment-service/knative/kafka-event-source.yaml
----
+
. Delete the event source

You should now be able to create event sources in the UI again.  If you see the error again you [red]#there might be some caching at play and you may need to REFRESH the page or otherwise invalidate the cache#

=== 500 errors

You may notice 500 errors, particularly if you send multiple requests under load:

image:500-errors.png[]

I believe this is because there is currently a race condition when the second request hits a pod where the payment topic (`producer` in the code) is not fully setup in the payment service (thus a null pointer).  Looks like the first exception happens in the `pass` function but this is caught in the handleCloudEvent function, only for the `fail` event to use the `producer` null pointer to try to log a failure at which time a new uncaught exception is raised.

If you set the concurrently limit to 1, you should be able to demonstration that this error doesn't happen with hey

=== Getting logs of Knative service

The epheral nature of the knative service can make it hard to capture logs of the service, particularly if you notice that the service had issues after it's gone.

Aside from setting up Elasticsearch to retain all logs, you can consider using `stern` in the background.  Using the .devcontainer that is run from within VSCode, you can have the following command running in a background terminal:

----
stern -l serving.knative.dev/service=payment
----

To see all the logs from revision 1 of the payment service (-1 represents the revision number I believe).  This command will include logs from all containers associated with the pod (such as `queue-proxy`).  If you only want the deployed code itself to log, add the `-c user-container` flag

=== Viewing and Modifying Order (MongoDB) Database

You cannot connect to the mongodb instance using the latest plain adminer container.  Instead you need to follow the special instructions below.  If you my version of adminer does not work for you, you can attempt to follow <<Updating your own Adminer image,these instructions>> for creating a new image yourself from the latest.

. Start port forwarding to the mongodb service
+
----
oc port-forward -n dev-demo-dev svc/order-database 27017:27017
----
+
. Run the modified adminer pod
** NOTE: `quay.io/mhildenb/myadminer:1.1` is a version 4.7.6 adminer container that I've updated to support this
+
----
docker run -p 8080:8080 -e ADMINER_DEFAULT_SERVER=docker.for.mac.localhost quay.io/mhildenb/myadminer:1.1
----
+
. Login as shown
+
image:adminer-mongo-password.png[]
+
. You should now have access to the mongo database with the ability to list and edit entries:
+
image:adminer-mongo-edit.png[]

==== Updating your own Adminer image

NOTE: It's possible to just install the mongodb elements to adminer:4.6.2 image as v4.6.2 is the last version of adminer that allows logging into a database without a user and a password

There are two reasons why the normal adminer image cannot connect to the mongo database:

1. It requires a newer version of php integration with MongoDB
2. The mongoDB is not setup with a user and a password (Adminer does not allow access to such databases by default for security reasons)

To update the latest adminer image to be able to connect to the userless MongoDB follow these instructions:

. Run an instance of the adminer container as follows:
+
----
docker run -it -u root --name my_adminer adminer:latest sh 
----
** NOTE: If an instance of the container is already running you can use the `docker exec -it` command instead
+
. Then from inside the container run
+
----
apk update
apk add autoconf gcc g++ make libffi-dev openssl-dev
pecl install mongodb
echo "extension=mongodb.so" > /usr/local/etc/php/conf.d/docker-php-ext-mongodb.ini
----
+
. Next add a plugin as per link:https://nerdpress.org/2019/10/23/adminer-for-sqlite-in-docker/[This site].  It will require you to create a login-password-less.php file in the `/var/www/html/plugins-enabled/` directory
+
[CONTENTS]
====
----
<?php
require_once('plugins/login-password-less.php');

/** Set allowed password
 * @param string result of password_hash
 */
return new AdminerLoginPasswordLess(
    $password_hash = password_hash("admin", PASSWORD_DEFAULT)
);
----
====
+
. now commit this container as a new image
+
----
docker commit my_adminer myadminer:1.1    
----

=== Insecure ImageRegistry ===

Might be solved as per link:https://github.com/knative/serving/issues/2136[here] but can't get the controller pod to take the new environment variable

Looks like it has something to do with the labels.  If the sha is used instead it seems to work properly.  You can find the sha like this:
----
$ oc get istag/payment:latest -o jsonpath='{.image.dockerImageReference}'
image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment@sha256:21ca1acc3f292b6e94fab82fe7a9cf7ff743e4a8c9459f711ffad125379cf3c7
----

And then apply it as a service like this:
----
kn service create payment --image $(oc get istag/payment:initial-build -o jsonpath='{.image.dockerImageReference}') --label "app.kubernetes.io/part-of=focus" --revision-name "{{.Service}}-{{.Generation}}" --annotation sidecar.istio.io/inject=false --force
----

----
oc port-forward <image-registry-pod> -n openshift-image-registry 5001:5000
----

To get the cert as a pem file, do this:
----
openssl s_client -showcerts -connect localhost:5001 </dev/null 2>/dev/null|openssl x509 -outform PEM >mycertfile.pem
----

== Appendix

=== Creating Topics in the UI ===

. While we're waiting for the creation of the cluster to complete, add 2 topics, one for `orders` and one for `payments`
** scroll right to go to the Kafka Topic
** Be sure to set partitions to `100`
+
image:kafka-topic-payments.png[]
+
. Go back to the details and scroll down to conditions and you will see the appropriate message regarding the state of the cluster
+
. Finally, switch to the Developer Perspective of that project to show the kafka resources spinning up

=== Copying OpenShift images to public repositories

If you have images that you've compiled on an OpenShift cluster and you want to pull them out of the local image stream to something like `quay.io`, you can use one of the following approaches to copy the images out of openshift.  Both use the `skopeo` command which is installed by default in the .devcontainer.  

For both examples, it assumes the copying of a payment service.  As such, note the following for the different variables:

* USER: your username for the public repository
* PASSWORD: your password or TOKEN for the public repository
* PROJECT: the project your image stream lives in (such as coolstore)
* IMAGE_DEST: Replace this with your repository, project, image-name, and version, example: `quay.io/mhildenb/homemade-serverless-native:1.0`: 

==== Image Registry is exposed publicly 

You need only run the following command:

----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --dest-creds "${USER}:${PASSWORD}" docker://$(oc get is payment -o jsonpath='{.status.publicDockerImageRepository}'):latest docker://{IMAGE_DEST}       
----

==== Image Registry is private

If instead you need to copy from an image registry that is not exposed outside the cluster, you must instead do the following:

. Port forward to openshift's internal image registry
+
----
oc port-forward svc/image-registry -n openshift-image-registry 5000:5000
----
+
. Then in a separate shell, run the following command
+
----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --src-tls-verify=false --dest-creds "${USER}:${PASSWORD}" docker://localhost:5000/${PROJECT}/payment:latest docker://{IMAGE_DEST}
----

=== Cut Parts of Demonstration

==== URL Invocation ====

In this section we want to show that the route created for the payment service allows us to invoke the payment service directly

. Go to the dev-demo-dev project
. Go to developer perspective
. Set Application to "Focus"
. Show the payment knative Service and zoom in on this in the browser window
. Setup Windows for Next Demonstration
** Split the browser window to have developer perspective on top and coolstore on bottom
** Open another shell from which you'll send the curl request
** Windows should look something like this:
+
image:window-setup-invoke.png[]
+
. Have that shown in the window
. Highlight (and copy) the route that is shown in the knative service
. In the bottom shell, set the `KNATIVE_ROUTE` variable
+
----
KNATIVE_ROUTE=<pasted value>
----
+
. Next invoke this command from the shell
+
----
curl -i -H 'Content-Type: application/json' -X POST --data-binary @$DEMO_HOME/example/order-payload.json $KNATIVE_ROUTE
----
+
. Point out that the service spins up and puts something in the payment queue.  But our order is still unprocessed

==== Extended CRW ====

. Go back to the CRW tab
+
image:crw-payment-service.png[]
+
. kbd:[CMD+P] and `PaymentResource` to open that file quickly
. Edit the file
** Add to SUCCESS and FAILURE messages a `(CRW Build)`
. Seed the m2 cache (select command from the right)
+
image:crw-seed-cache.png[]
+
. Select `Start Quarkus in Dev Mode`
+
. Wait for compilation to finish
+
. Set a breakpoint at the top of the "HandleCloudEvent" method
. Go to the debug pane
. Click on the green play button
. Click `quarkus-development-server` on the right
** This should open a separate tab that gives the default / page
. Copy the URL of the tab to `CRW_ROUTE`
. From the blue shell, run the following command
+
----
curl -X POST -H "Content-Type: application/json" -d @$DEMO_HOME/example/order-payload.json $CRW_ROUTE
----
+ 
. Go back to the CRW tab and see that it's waiting in the debugger


==== Scaling Support ====

===== Using `hey` =====

Hey (or any tool that can generate http request concurrently) is the most accurate way to demonstrate scaling.  If you have 1 request per pod and you make 100 hey calls, you generally get 100 pods scaled up.  This is not generally true of the kafka queue approach due to the additional complexity of latency around posting messages to a topic and having these generate HTTP requests to the "sink" service

. Run this command to simulate orders coming in from coolstore
+
----
hey -n 100 -c 100 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $(oc get rt payment -n dev-demo-dev -o jsonpath='{.status.url}')
----
+
. Notice that close to 100 pods spin up
. Review `hey` report
+
image:hey-report.png[]

==== Tekton Pipeline UI ====

_this was from the middle part when we'd show off retry due to a configuration error_

. The pipeline fail (this is due to the service account not having the proper permissions) and you will see this error in the logs
+
image:oc-error.png[]
+
. From a shell, run the following commands to update the permissions for the pipeline account
+
----
oc adm policy add-cluster-role-to-user -n dev-demo-stage kn-deployer system:serviceaccount:dev-demo-dev:pipeline
----
+
. And rerun the pipeline by going to Actions > Rerun
+